---
title: "PML Course Project"
output: html_document
---

## Summary

Use the data from a weight lifting dataset to classify each of the 20 test observations into 1 of the 5 categories.

## Preprocessing

Load the data and review the column summaries. There are 160 columns. The columns that were mostly empty were identified and removed. Columns 1 through 7 were metadata like "username" and timestamps. They were removed because they weren't measurements that could classify the weight lifting actions. Next, factor columns were converted to numeric so that they could be trained on. Then columns containing NAs were removed. This is justified by noting that the 20 test observations have NAs in these columns as well.

<b>This left 52 numeric columns.</b>

The column names are reviewed to ensure we are only keeping columns relevant to the movement of the observed exerciser.


```{r warning=FALSE}
d0 <- read.csv("pml-training.csv")
dim(d0)
ex <- c(14, 17, 26, 89, 92, 101, 127, 130, 139, 160)
d1 <- d0[,-ex]
d1 <- d1[,-(1:7)]
for(i in which(sapply(d1, is.factor))){ d1[,i] <- as.numeric(as.character(d1[,i])) }
pna <- function (x) { a <- sum(is.na(x)); b <- length (x); c(a, b, a/b) }
nas <- t(sapply(d1, pna))
d2 <- d1[,(nas[,3] < .9)]
dim(d2)
```

Next, the "classe" column is restored and the data is broken into a training and test set. <b>We set aside 30% of the data for cross validation.</b>

```{r results='hide', message=FALSE}
d3 <- cbind(d2, classe = d0$classe)
library(caret)
inTrain <- createDataPartition(y = d3$classe, p=0.7, list = FALSE)
training <- d3[inTrain,]
testing <- d3[-inTrain,]
```

A table of the distribution of classifications (A-E) is printed to ensure they are well-represented. <b>All 5 categories exist almost equally in both sets</b> - so it is reasonable to conclude that the predition algorithm will be able to identify each category. 

```{r}
table(training$classe)
table(testing$classe)
```

## Training

Training is done with a <b>random forest</b>. (It was attempted with "rpart" first which resulted in very poor accuracy.)

```{r eval=FALSE}
fit <- train(classe ~ ., method="rf", data=training)
pred <- predict(fit, testing)
confusionMatrix(testing$classe, pred)
fit$finalModel
```

## Error Rate and Cross Validation

The model fit for the random forest stated an out-of-bag error of <b>0.63%.</b>, which is an accurate prediction of our <b>Out-of-Sample Error Rate</b>! 

```{r}
# OOB estimate of  error rate: 0.63%
```

Cross validating results in an <b>in-sample error rate of 0.51%</b>.

```{r}
#           Reference
# Prediction    A    B    C    D    E
#          A 1395    0    0    0    0
#          B    5  942    2    0    0
#          C    0    5  849    1    0
#          D    0    0   11  793    0
#          E    0    0    0    1  900
#
# Overall Statistics
#                                          
#              Accuracy : 0.9949 
```

## Principle Components Plot

A principle components analysis on the 52 numeric columns reveals that the first <b>7 components are responsible for > 91% of the variance</b> and the first two for 47%. We graph these components and color by category. This reveals 5 clusters.

```{r}
p <- prcomp(d3[,-53], center = TRUE, scale = TRUE)
plot(p$x[,1], p$x[,2], col=d3$classe)
```

## End

Congrats on getting to the end of the DSS!
